{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n\ntrain_df.drop(columns=[\"Id\"], inplace=True)\n\ntrain_df.dropna(subset=[\"SalePrice\"], inplace=True)\n\nX = train_df.drop(columns=[\"SalePrice\"])\ny = train_df[\"SalePrice\"]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:43:51.468309Z","iopub.execute_input":"2025-04-12T18:43:51.468638Z","iopub.status.idle":"2025-04-12T18:43:51.504273Z","shell.execute_reply.started":"2025-04-12T18:43:51.468610Z","shell.execute_reply":"2025-04-12T18:43:51.503409Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# Cleaning:","metadata":{}},{"cell_type":"code","source":"# Cleaning\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\n\nclass DropHighNaNColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.3):\n        self.threshold = threshold\n        self.columns_to_keep_ = None\n\n    def fit(self, X, y=None):\n        nan_ratio = pd.isnull(X).mean()\n        self.columns_to_keep_ = nan_ratio[nan_ratio <= self.threshold].index\n        return self\n\n    def transform(self, X):\n        return X[self.columns_to_keep_]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:44:02.420127Z","iopub.execute_input":"2025-04-12T18:44:02.420463Z","iopub.status.idle":"2025-04-12T18:44:02.427068Z","shell.execute_reply.started":"2025-04-12T18:44:02.420443Z","shell.execute_reply":"2025-04-12T18:44:02.426015Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# Feature Engineering:","metadata":{}},{"cell_type":"code","source":"\n\n# Feature Engineering\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnum_pipe = Pipeline([\n  ('imputer', SimpleImputer(strategy='median')),\n  ('scaler', StandardScaler())\n])\n\ncat_pipe = Pipeline([\n  ('imputer', SimpleImputer(strategy='most_frequent')),\n  ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer([\n    ('num', num_pipe, make_column_selector(dtype_include=['int64', 'float64'])),\n    ('cat', cat_pipe, make_column_selector(dtype_include=['object']))\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:44:27.256428Z","iopub.execute_input":"2025-04-12T18:44:27.256727Z","iopub.status.idle":"2025-04-12T18:44:27.263314Z","shell.execute_reply.started":"2025-04-12T18:44:27.256707Z","shell.execute_reply":"2025-04-12T18:44:27.262506Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\n\n# Define the model pipeline with options for different models\nmodel_pipeline = Pipeline([\n    ('clean', DropHighNaNColumns()),\n    ('feature_engineering', preprocessor),\n    # ('feature_selection', SelectKBest(score_func=mutual_info_regression, k=20)),\n    ('model', Ridge())  # Default model, will change with grid search\n])\n\nparam_grid = [\n    {\n        'clean__threshold': [0.1, 0.2, 0.5],\n        'model': [Ridge()],\n        'model__alpha': [0.01, 0.1, 1.0, 10.0]\n    },\n    {\n        'clean__threshold': [0.1, 0.2],\n        'model': [RandomForestRegressor()],\n        'model__n_estimators': [100, 200],\n        'model__max_depth': [5, 10]\n    },\n    {\n        'clean__threshold': [0.1],\n        'model': [xgb.XGBRegressor(eval_metric='rmse', verbosity=0)],\n        'model__n_estimators': [100, 200],\n        'model__learning_rate': [0.01, 0.1],\n        'model__max_depth': [3, 6]\n    }\n]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ngrid = GridSearchCV(model_pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error')\ngrid.fit(X_train, y_train)\n\nbest_model = grid.best_estimator_\nval_preds = best_model.predict(X_test)\n# Calculate RMSE\nval_rmse = np.sqrt(mean_squared_error(np.log1p(y_test), np.log1p(val_preds)))\nprint(\"Validation RMSE (log scale):\", val_rmse)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:46:49.291847Z","iopub.execute_input":"2025-04-12T18:46:49.292145Z","iopub.status.idle":"2025-04-12T18:50:10.547716Z","shell.execute_reply.started":"2025-04-12T18:46:49.292124Z","shell.execute_reply":"2025-04-12T18:50:10.547046Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n60 fits failed out of a total of 140.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\", line 129, in _solve_sparse_cg\n    coefs[i], info = sp_linalg.cg(\n                     ^^^^^^^^^^^^^\nTypeError: cg() got an unexpected keyword argument 'tol'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\", line 1134, in fit\n    return super().fit(X, y, sample_weight=sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\", line 900, in fit\n    self.coef_, self.n_iter_ = _ridge_regression(\n                               ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\", line 671, in _ridge_regression\n    coef = _solve_sparse_cg(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\", line 134, in _solve_sparse_cg\n    coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter, tol=tol)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: cg() got an unexpected keyword argument 'tol'\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n             nan             nan             nan             nan\n             nan             nan             nan             nan\n -33406.28989214 -33466.81829285 -30715.09190585 -29996.7476547\n -33726.90526433 -33762.83121885 -30823.12031655 -30749.37984753\n -45724.85633295 -35447.07093082 -42885.2787554  -33609.85897134\n -27966.06945538 -27414.2013983  -29046.91201165 -28959.21884356]\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation RMSE (log scale): 0.13625557340876357\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Logging\nimport mlflow\nimport dagshub\n\ndagshub.init(repo_owner='ashar-22', repo_name='hw01ml', mlflow=True)\n# End any previous run if it's still active\nif mlflow.active_run():\n    mlflow.end_run()\n\nwith mlflow.start_run():\n\n  mlflow.log_param(\"model_type\", type(grid.best_estimator_.named_steps['model']).__name__)\n  mlflow.log_param(\"best_alpha\", grid.best_params_.get('model__alpha', 'N/A'))\n  mlflow.log_param(\"best_nan_threshold\", grid.best_params_['clean__threshold'])\n  mlflow.log_param(\"best_n_estimators\", grid.best_params_.get('model__n_estimators', 'N/A'))\n  mlflow.log_param(\"best_max_depth\", grid.best_params_.get('model__max_depth', 'N/A'))\n  mlflow.log_param(\"best_learning_rate\", grid.best_params_.get('model__learning_rate', 'N/A'))\n  mlflow.log_metric(\"val_rmse_log\", val_rmse)\n\ninput_example = X_train.iloc[:5]  # or use .head() ‚Äî a small sample input\n\nmlflow.sklearn.log_model(\n    best_model,\n    artifact_path=\"model\",\n    input_example=input_example\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T18:50:38.610078Z","iopub.execute_input":"2025-04-12T18:50:38.610460Z","iopub.status.idle":"2025-04-12T18:50:59.290305Z","shell.execute_reply.started":"2025-04-12T18:50:38.610430Z","shell.execute_reply":"2025-04-12T18:50:59.289387Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"ashar-22/hw01ml\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ashar-22/hw01ml\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository ashar-\u001b[1;36m22\u001b[0m/hw01ml initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ashar-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>/hw01ml initialized!\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"üèÉ View run shivering-fly-325 at: https://dagshub.com/ashar-22/hw01ml.mlflow/#/experiments/0/runs/847996015e4545f3873b5cdb5a03d074\nüß™ View experiment at: https://dagshub.com/ashar-22/hw01ml.mlflow/#/experiments/0\nüèÉ View run welcoming-hound-3 at: https://dagshub.com/ashar-22/hw01ml.mlflow/#/experiments/0/runs/50c94704c2874675ab043143343acf0e\nüß™ View experiment at: https://dagshub.com/ashar-22/hw01ml.mlflow/#/experiments/0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"<mlflow.models.model.ModelInfo at 0x7c3f5a9b2550>"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\") \n\nids = df[\"Id\"]\n\nmodel = best_model\n\npredictions = model.predict(df)\n\nsubmission = pd.DataFrame({\n    \"Id\": ids,\n    \"SalePrice\": predictions\n})\n\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n\nprint(\"‚úÖ submission.csv saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T19:12:37.152138Z","iopub.execute_input":"2025-04-12T19:12:37.152506Z","iopub.status.idle":"2025-04-12T19:12:37.212616Z","shell.execute_reply.started":"2025-04-12T19:12:37.152481Z","shell.execute_reply":"2025-04-12T19:12:37.211677Z"}},"outputs":[{"name":"stdout","text":"‚úÖ submission.csv saved!\n","output_type":"stream"}],"execution_count":72}]}